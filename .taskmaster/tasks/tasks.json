{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Setup Project Repository and Development Environment",
        "description": "Initialize project repository with proper structure, configure Python environment, and set up development dependencies for GraphRAG retrieval system",
        "details": "Create project structure:\n```\ngraphrag-retrieval/\nâ”œâ”€â”€ src/\nâ”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”œâ”€â”€ retrieval/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â”œâ”€â”€ graph_rag.py\nâ”‚   â”‚   â””â”€â”€ base.py\nâ”‚   â”œâ”€â”€ database/\nâ”‚   â”‚   â”œâ”€â”€ __init__.py\nâ”‚   â”‚   â””â”€â”€ neo4j_client.py\nâ”‚   â””â”€â”€ models/\nâ”‚       â”œâ”€â”€ __init__.py\nâ”‚       â””â”€â”€ data_models.py\nâ”œâ”€â”€ examples/\nâ”‚   â””â”€â”€ example_data_loader.py\nâ”œâ”€â”€ tests/\nâ”œâ”€â”€ docker-compose.yml\nâ”œâ”€â”€ requirements.txt\nâ”œâ”€â”€ setup.py\nâ””â”€â”€ README.md\n```\nSet up requirements.txt with:\n- neo4j==5.14.0\n- pydantic==2.5.0\n- python-dotenv==1.0.0\n- pytest==7.4.3\n- pytest-asyncio==0.21.1",
        "testStrategy": "Verify project structure is created correctly, all dependencies install without conflicts, and basic import tests pass for the module structure",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Git Repository and Basic Project Structure",
            "description": "Create the project root directory, initialize Git repository, and set up the basic folder structure with all necessary directories",
            "dependencies": [],
            "details": "Create graphrag-retrieval directory, initialize git with 'git init', create .gitignore file with Python-specific exclusions (*.pyc, __pycache__, .env, venv/, .pytest_cache/), and create all subdirectories: src/, src/retrieval/, src/database/, src/models/, examples/, and tests/",
            "status": "done",
            "testStrategy": "Verify directory structure exists using os.path.exists() for each directory, ensure .git folder is present, and validate .gitignore contains necessary patterns"
          },
          {
            "id": 2,
            "title": "Create Python Package Structure with __init__.py Files",
            "description": "Add __init__.py files to all Python packages to make them importable modules and set up proper package initialization",
            "dependencies": [
              1
            ],
            "details": "Create empty __init__.py files in: src/, src/retrieval/, src/database/, and src/models/. Add version information to src/__init__.py with __version__ = '0.1.0'. Include basic docstrings describing each package's purpose",
            "status": "done",
            "testStrategy": "Import each package using Python interpreter to ensure no import errors, verify __version__ is accessible from src package"
          },
          {
            "id": 3,
            "title": "Set Up Python Virtual Environment and Dependencies",
            "description": "Create Python virtual environment, configure requirements.txt with all specified dependencies, and install them",
            "dependencies": [
              1
            ],
            "details": "Create requirements.txt with exact versions: neo4j==5.14.0, pydantic==2.5.0, python-dotenv==1.0.0, pytest==7.4.3, pytest-asyncio==0.21.1. Create virtual environment with 'python -m venv venv', activate it, and install dependencies with 'pip install -r requirements.txt'. Add requirements-dev.txt for development dependencies like black, flake8, mypy",
            "status": "done",
            "testStrategy": "Run 'pip freeze' to verify all packages are installed with correct versions, import each package in Python to ensure no missing dependencies"
          },
          {
            "id": 4,
            "title": "Create Core Python Module Files and Base Classes",
            "description": "Create the skeleton Python files for core modules with basic class definitions and placeholder methods",
            "dependencies": [
              2
            ],
            "details": "Create src/retrieval/base.py with abstract base class for retrieval systems, src/retrieval/graph_rag.py with GraphRAG class skeleton inheriting from base, src/database/neo4j_client.py with Neo4jClient class for database connections, src/models/data_models.py with Pydantic models for data structures, and examples/example_data_loader.py with sample data loading logic",
            "status": "done",
            "testStrategy": "Ensure all files can be imported without syntax errors, run 'python -m py_compile' on each file to check for compilation errors"
          },
          {
            "id": 5,
            "title": "Configure Project Metadata and Development Tools",
            "description": "Set up setup.py for package installation, create docker-compose.yml for Neo4j database, write README.md with project documentation, and configure development tools",
            "dependencies": [
              3,
              4
            ],
            "details": "Create setup.py with project metadata, entry points, and package discovery configuration. Write docker-compose.yml with Neo4j service configuration (image: neo4j:5, ports: 7474:7474, 7687:7687, environment variables for auth). Create comprehensive README.md with project description, installation instructions, and usage examples. Add .env.example file with required environment variables template",
            "status": "done",
            "testStrategy": "Run 'pip install -e .' to test editable installation, validate docker-compose.yml with 'docker-compose config', ensure README.md renders correctly in markdown preview"
          }
        ]
      },
      {
        "id": 2,
        "title": "Configure Neo4j Database Infrastructure",
        "description": "Set up Neo4j database instance using Docker, configure connection settings, and establish database connectivity",
        "details": "Create docker-compose.yml:\n```yaml\nversion: '3.8'\nservices:\n  neo4j:\n    image: neo4j:5.14.0\n    ports:\n      - '7474:7474'\n      - '7687:7687'\n    environment:\n      - NEO4J_AUTH=neo4j/password123\n      - NEO4J_PLUGINS=[\"apoc\", \"graph-data-science\"]\n    volumes:\n      - neo4j_data:/data\n      - neo4j_logs:/logs\nvolumes:\n  neo4j_data:\n  neo4j_logs:\n```\nImplement Neo4j client class with connection pooling and error handling:\n```python\nfrom neo4j import GraphDatabase\nfrom typing import Optional\nimport os\n\nclass Neo4jClient:\n    def __init__(self, uri: str, user: str, password: str):\n        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n    \n    def close(self):\n        self.driver.close()\n    \n    def verify_connectivity(self):\n        with self.driver.session() as session:\n            result = session.run(\"RETURN 1\")\n            return result.single()[0] == 1\n```",
        "testStrategy": "Test database connectivity, verify Neo4j instance is accessible via both web interface (7474) and bolt protocol (7687), run basic Cypher queries",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Define Data Models for Compliance Domain",
        "description": "Create Pydantic models for Federal Regulation, Agency Guidance, Enforcement Action, and Compliance Topic nodes as specified in PRD",
        "details": "Implement data models using Pydantic:\n```python\nfrom pydantic import BaseModel, Field\nfrom datetime import date\nfrom typing import Optional, List\nfrom enum import Enum\n\nclass Category(str, Enum):\n    FINANCIAL = \"financial\"\n    HEALTHCARE = \"healthcare\"\n    ENVIRONMENTAL = \"environmental\"\n    DATA_PRIVACY = \"data_privacy\"\n    OTHER = \"other\"\n\nclass FederalRegulation(BaseModel):\n    name: str\n    citation: str\n    description: str\n    effective_date: date\n    full_text: Optional[str] = None\n    category: Category\n    \nclass AgencyGuidance(BaseModel):\n    title: str\n    agency: str\n    date_issued: date\n    summary: str\n    reference_number: str\n    category: Category\n    \nclass EnforcementAction(BaseModel):\n    title: str\n    agency: str\n    date: date\n    summary: str\n    docket_number: str\n    outcome: str\n    category: Category\n    \nclass ComplianceTopic(BaseModel):\n    name: str\n    description: str\n    related_regulations: List[str]\n    category: Category\n```",
        "testStrategy": "Unit tests for model validation, serialization/deserialization, field constraints, and enum validation. Test edge cases with missing optional fields",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Implement Example Data Loader",
        "description": "Create data loader to populate Neo4j with sample compliance data for each node type, establishing relationships between nodes",
        "details": "Create example_data_loader.py with sample data:\n```python\nfrom datetime import date\nfrom src.models.data_models import *\nfrom src.database.neo4j_client import Neo4jClient\n\nclass ExampleDataLoader:\n    def __init__(self, neo4j_client: Neo4jClient):\n        self.client = neo4j_client\n        \n    def load_sample_data(self):\n        # Clear existing data\n        self._clear_database()\n        \n        # Create sample nodes\n        regulations = [\n            FederalRegulation(\n                name=\"GDPR\",\n                citation=\"Regulation (EU) 2016/679\",\n                description=\"General Data Protection Regulation\",\n                effective_date=date(2018, 5, 25),\n                category=Category.DATA_PRIVACY\n            ),\n            # Add more examples\n        ]\n        \n        # Create nodes and relationships\n        self._create_regulations(regulations)\n        self._create_relationships()\n        \n    def _create_regulations(self, regulations):\n        with self.client.driver.session() as session:\n            for reg in regulations:\n                session.run(\n                    \"CREATE (r:FederalRegulation {name: $name, citation: $citation, \"\n                    \"description: $description, effective_date: $effective_date, \"\n                    \"category: $category})\",\n                    **reg.dict()\n                )\n```",
        "testStrategy": "Verify all sample data is loaded correctly, check node counts match expected values, validate relationships are created properly using Cypher queries",
        "priority": "medium",
        "dependencies": [
          2,
          3
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Design Base Retrieval Interface",
        "description": "Create abstract base class defining the retrieval interface to ensure extensibility for future retrieval methods",
        "details": "Implement base retrieval interface:\n```python\nfrom abc import ABC, abstractmethod\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass RetrievalQuery:\n    query_text: str\n    filters: Optional[Dict[str, Any]] = None\n    limit: int = 10\n    \n@dataclass\nclass RetrievalResult:\n    content: str\n    metadata: Dict[str, Any]\n    score: float\n    source_type: str\n    \nclass BaseRetriever(ABC):\n    @abstractmethod\n    def retrieve(self, query: RetrievalQuery) -> List[RetrievalResult]:\n        \"\"\"Retrieve relevant information based on query\"\"\"\n        pass\n        \n    @abstractmethod\n    def initialize(self, config: Dict[str, Any]) -> None:\n        \"\"\"Initialize retriever with configuration\"\"\"\n        pass\n```",
        "testStrategy": "Test interface contracts, ensure abstract methods cannot be instantiated directly, verify subclasses must implement all abstract methods",
        "priority": "high",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement Core GraphRAG Retrieval Logic",
        "description": "Build the main GraphRAG retrieval implementation using Neo4j, including graph traversal, relevance scoring, and result aggregation",
        "details": "Implement GraphRAG retriever:\n```python\nfrom src.retrieval.base import BaseRetriever, RetrievalQuery, RetrievalResult\nfrom src.database.neo4j_client import Neo4jClient\nimport numpy as np\nfrom typing import List, Dict, Any\n\nclass GraphRAGRetriever(BaseRetriever):\n    def __init__(self):\n        self.client = None\n        self.embeddings_cache = {}\n        \n    def initialize(self, config: Dict[str, Any]) -> None:\n        self.client = Neo4jClient(\n            uri=config['neo4j_uri'],\n            user=config['neo4j_user'],\n            password=config['neo4j_password']\n        )\n        \n    def retrieve(self, query: RetrievalQuery) -> List[RetrievalResult]:\n        # 1. Generate query embedding (simplified for MVP)\n        query_embedding = self._generate_embedding(query.query_text)\n        \n        # 2. Find relevant nodes using graph traversal\n        relevant_nodes = self._graph_search(query_embedding, query.filters)\n        \n        # 3. Expand context using graph relationships\n        expanded_context = self._expand_context(relevant_nodes)\n        \n        # 4. Score and rank results\n        scored_results = self._score_results(expanded_context, query_embedding)\n        \n        # 5. Format and return top results\n        return self._format_results(scored_results[:query.limit])\n        \n    def _graph_search(self, embedding, filters):\n        cypher_query = \"\"\"\n        MATCH (n)\n        WHERE n.description CONTAINS $search_text\n        OPTIONAL MATCH (n)-[r]-(connected)\n        RETURN n, collect(connected) as connections\n        LIMIT 20\n        \"\"\"\n        # Execute search with filters\n        pass\n```",
        "testStrategy": "Test retrieval with various query types, verify graph traversal logic, validate scoring algorithm, ensure results are properly ranked and formatted",
        "priority": "high",
        "dependencies": [
          2,
          3,
          5
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Create Module API Interface",
        "description": "Design and implement a clean API interface for the retrieval module that can be easily imported and used by other services",
        "details": "Create main module interface:\n```python\n# src/retrieval/__init__.py\nfrom typing import Dict, Any, Optional\nfrom .graph_rag import GraphRAGRetriever\nfrom .base import RetrievalQuery, RetrievalResult\n\nclass RetrievalModule:\n    def __init__(self, retriever_type: str = 'graphrag'):\n        self.retriever_type = retriever_type\n        self.retriever = None\n        self._initialized = False\n        \n    def initialize(self, config: Dict[str, Any]) -> None:\n        \"\"\"Initialize the retrieval module with configuration\"\"\"\n        if self.retriever_type == 'graphrag':\n            self.retriever = GraphRAGRetriever()\n        else:\n            raise ValueError(f\"Unknown retriever type: {self.retriever_type}\")\n            \n        self.retriever.initialize(config)\n        self._initialized = True\n        \n    def retrieve(self, query_text: str, filters: Optional[Dict] = None, limit: int = 10):\n        \"\"\"Main retrieval method\"\"\"\n        if not self._initialized:\n            raise RuntimeError(\"Module not initialized. Call initialize() first.\")\n            \n        query = RetrievalQuery(\n            query_text=query_text,\n            filters=filters,\n            limit=limit\n        )\n        return self.retriever.retrieve(query)\n        \n# Export main classes\n__all__ = ['RetrievalModule', 'RetrievalQuery', 'RetrievalResult']\n```",
        "testStrategy": "Test module import functionality, verify initialization flow, test error handling for uninitialized usage, validate API contract with mock services",
        "priority": "high",
        "dependencies": [
          6
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Configuration Management",
        "description": "Create configuration system for managing Neo4j connections, retrieval parameters, and future extensibility options",
        "details": "Implement configuration management:\n```python\n# src/config.py\nfrom pydantic import BaseSettings, Field\nfrom typing import Optional, Dict, Any\n\nclass Neo4jConfig(BaseSettings):\n    uri: str = Field(default=\"bolt://localhost:7687\")\n    user: str = Field(default=\"neo4j\")\n    password: str = Field(default=\"password123\")\n    \n    class Config:\n        env_prefix = \"NEO4J_\"\n        \nclass RetrievalConfig(BaseSettings):\n    default_limit: int = 10\n    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n    score_threshold: float = 0.7\n    expand_hops: int = 2  # Graph expansion depth\n    \n    class Config:\n        env_prefix = \"RETRIEVAL_\"\n        \nclass AppConfig:\n    def __init__(self):\n        self.neo4j = Neo4jConfig()\n        self.retrieval = RetrievalConfig()\n        \n    def to_dict(self) -> Dict[str, Any]:\n        return {\n            'neo4j_uri': self.neo4j.uri,\n            'neo4j_user': self.neo4j.user,\n            'neo4j_password': self.neo4j.password,\n            'retrieval_config': self.retrieval.dict()\n        }\n```",
        "testStrategy": "Test configuration loading from environment variables, validate default values, test configuration validation, ensure sensitive data is handled securely",
        "priority": "medium",
        "dependencies": [
          1
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Build Comprehensive Test Suite",
        "description": "Create unit tests, integration tests, and end-to-end tests for the retrieval module ensuring reliability and maintainability",
        "details": "Implement test suite:\n```python\n# tests/test_retrieval.py\nimport pytest\nfrom src.retrieval import RetrievalModule\nfrom src.database.neo4j_client import Neo4jClient\nfrom src.config import AppConfig\n\n@pytest.fixture\ndef retrieval_module():\n    config = AppConfig()\n    module = RetrievalModule('graphrag')\n    module.initialize(config.to_dict())\n    return module\n    \ndef test_basic_retrieval(retrieval_module):\n    results = retrieval_module.retrieve(\n        \"Find regulations about data privacy\",\n        filters={'category': 'data_privacy'},\n        limit=5\n    )\n    assert len(results) <= 5\n    assert all(r.score >= 0 for r in results)\n    \ndef test_graph_expansion():\n    # Test that related nodes are included in results\n    pass\n    \ndef test_error_handling():\n    module = RetrievalModule()\n    with pytest.raises(RuntimeError):\n        module.retrieve(\"test query\")\n        \n# tests/test_integration.py\n@pytest.mark.integration\ndef test_neo4j_connection():\n    # Test actual Neo4j connectivity\n    pass\n```",
        "testStrategy": "Run pytest with coverage reports, ensure >80% code coverage, test both happy paths and error scenarios, validate integration with Neo4j",
        "priority": "medium",
        "dependencies": [
          7
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Create Documentation and Usage Examples",
        "description": "Write comprehensive documentation including API reference, usage examples, and deployment guide for developers integrating the module",
        "details": "Create documentation structure:\n```markdown\n# GraphRAG Retrieval Module\n\n## Installation\n```bash\npip install -e .\n```\n\n## Quick Start\n```python\nfrom graphrag_retrieval import RetrievalModule\n\n# Initialize module\nmodule = RetrievalModule('graphrag')\nmodule.initialize({\n    'neo4j_uri': 'bolt://localhost:7687',\n    'neo4j_user': 'neo4j',\n    'neo4j_password': 'password123'\n})\n\n# Perform retrieval\nresults = module.retrieve(\n    query_text=\"Find all GDPR compliance requirements\",\n    filters={'category': 'data_privacy'},\n    limit=10\n)\n\nfor result in results:\n    print(f\"Score: {result.score}\")\n    print(f\"Content: {result.content}\")\n    print(f\"Source: {result.source_type}\")\n```\n\n## API Reference\n[Document all public methods]\n\n## Extending the Module\n[Guide for adding new retrieval methods]\n```\n\nCreate example notebooks demonstrating various use cases",
        "testStrategy": "Validate all code examples run without errors, ensure documentation is accurate and complete, test example scripts in fresh environment",
        "priority": "low",
        "dependencies": [
          7,
          9
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Replace OpenAI-based fact-checking backend with GraphRAG-powered API",
        "description": "Update the fact-check project to use the GraphRAG-powered fact-checking API from factblock-retrieval instead of direct OpenAI calls, maintaining compatibility with existing endpoints",
        "details": "Modify /fact-check/api/fact_check_api.py to integrate with the new GraphRAG endpoint:\n\n1. Update imports and configuration:\n```python\n# Remove OpenAI imports\n# from openai import OpenAI\n# client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n\n# Add new GraphRAG client\nimport requests\nfrom typing import Dict, Any, Optional\n\nGRAPHRAG_API_URL = os.getenv('GRAPHRAG_API_URL', 'http://localhost:8000/fact-check-graphrag')\n```\n\n2. Replace OpenAI fact-checking function:\n```python\ndef fact_check_with_graphrag(claim: str, context: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Send fact-check request to GraphRAG API\"\"\"\n    try:\n        payload = {\n            'claim': claim,\n            'context': context,\n            'include_sources': True\n        }\n        \n        response = requests.post(\n            GRAPHRAG_API_URL,\n            json=payload,\n            headers={'Content-Type': 'application/json'},\n            timeout=30\n        )\n        \n        if response.status_code == 200:\n            return response.json()\n        else:\n            raise Exception(f'GraphRAG API error: {response.status_code}')\n    except Exception as e:\n        # Fallback or error handling\n        return {\n            'verdict': 'unknown',\n            'confidence': 0.0,\n            'explanation': f'Error connecting to GraphRAG service: {str(e)}',\n            'sources': []\n        }\n```\n\n3. Update existing fact-check endpoints to use new function:\n```python\n@app.post('/fact-check')\nasync def check_fact(request: FactCheckRequest):\n    # Previous: result = check_with_openai(request.claim)\n    result = fact_check_with_graphrag(\n        claim=request.claim,\n        context=request.context\n    )\n    \n    return FactCheckResponse(\n        claim=request.claim,\n        verdict=result['verdict'],\n        confidence=result['confidence'],\n        explanation=result['explanation'],\n        sources=result.get('sources', [])\n    )\n```\n\n4. Add response mapping for compatibility:\n```python\ndef map_graphrag_response(graphrag_result: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Map GraphRAG response to existing API format\"\"\"\n    return {\n        'verdict': graphrag_result.get('verdict', 'unknown'),\n        'confidence': graphrag_result.get('confidence_score', 0.0),\n        'explanation': graphrag_result.get('explanation', ''),\n        'sources': [\n            {\n                'title': source.get('title', ''),\n                'url': source.get('url', ''),\n                'relevance': source.get('relevance_score', 0.0)\n            }\n            for source in graphrag_result.get('sources', [])\n        ],\n        'metadata': {\n            'model': 'graphrag',\n            'processing_time': graphrag_result.get('processing_time', 0)\n        }\n    }\n```\n\n5. Update environment configuration:\n```bash\n# .env file\nGRAPHRAG_API_URL=http://factblock-retrieval:8000/fact-check-graphrag\n# Remove or comment out: OPENAI_API_KEY=...\n```\n\n6. Add retry logic and circuit breaker:\n```python\nfrom tenacity import retry, stop_after_attempt, wait_exponential\n\n@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\ndef fact_check_with_retry(claim: str, context: Optional[str] = None) -> Dict[str, Any]:\n    return fact_check_with_graphrag(claim, context)\n```",
        "testStrategy": "1. Unit test the new GraphRAG integration function with mocked HTTP responses\n2. Test error handling when GraphRAG service is unavailable\n3. Verify response mapping maintains backward compatibility with existing API consumers\n4. Integration test with actual GraphRAG service running locally\n5. Load test to ensure performance is comparable or better than OpenAI implementation\n6. Test retry logic and timeout handling\n7. Validate that all existing fact-check endpoints continue to work with the new backend\n8. Test with various claim types (simple facts, complex statements, claims with context)\n9. Verify source attribution is properly passed through from GraphRAG\n10. Run existing API test suite to ensure no regression",
        "status": "pending",
        "dependencies": [
          7,
          10
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 12,
        "title": "Enhance Streamlit UI with GraphRAG compliance evidence and source citations",
        "description": "Update the /fact-check Streamlit UI to display GraphRAG compliance evidence, source citations, regulatory document references, and source reliability scores for investor demonstrations",
        "details": "Modify streamlit_app.py to integrate with the GraphRAG-powered fact-checking API and display enhanced results:\n\n1. Update the UI layout to show compliance evidence:\n```python\nimport streamlit as st\nimport requests\nimport json\nfrom typing import Dict, List, Any\n\n# Add new UI components for evidence display\ndef display_fact_check_results(results: Dict[str, Any]):\n    st.header(\"Fact Check Results\")\n    \n    # Overall verdict\n    verdict = results.get('verdict', 'Unknown')\n    confidence = results.get('confidence', 0)\n    \n    col1, col2 = st.columns(2)\n    with col1:\n        st.metric(\"Verdict\", verdict, f\"{confidence:.1%} confidence\")\n    with col2:\n        reliability_score = results.get('source_reliability_score', 0)\n        st.metric(\"Source Reliability\", f\"{reliability_score:.1%}\")\n    \n    # Compliance Evidence Section\n    st.subheader(\"ðŸ“‹ Compliance Evidence\")\n    compliance_evidence = results.get('compliance_evidence', [])\n    \n    for evidence in compliance_evidence:\n        with st.expander(f\"ðŸ” {evidence.get('regulation', 'Unknown Regulation')}\":\n            st.write(f\"**Requirement:** {evidence.get('requirement', 'N/A')}\")\n            st.write(f\"**Compliance Status:** {evidence.get('status', 'Unknown')}\")\n            st.write(f\"**Evidence:** {evidence.get('evidence_text', 'No evidence available')}\")\n            \n            # Show related documents\n            if 'source_documents' in evidence:\n                st.write(\"**Source Documents:**\")\n                for doc in evidence['source_documents']:\n                    st.write(f\"- [{doc['title']}]({doc['url']}) - Page {doc.get('page', 'N/A')}\")\n```\n\n2. Add source citation display:\n```python\n# Source Citations Section\ndef display_source_citations(results: Dict[str, Any]):\n    st.subheader(\"ðŸ“š Source Citations\")\n    \n    citations = results.get('citations', [])\n    if not citations:\n        st.info(\"No citations available\")\n        return\n    \n    # Group citations by type\n    regulatory_docs = [c for c in citations if c.get('type') == 'regulatory']\n    industry_standards = [c for c in citations if c.get('type') == 'standard']\n    other_sources = [c for c in citations if c.get('type') not in ['regulatory', 'standard']]\n    \n    # Display regulatory documents\n    if regulatory_docs:\n        st.write(\"**Regulatory Documents:**\")\n        for doc in regulatory_docs:\n            reliability = doc.get('reliability_score', 0)\n            color = 'green' if reliability > 0.8 else 'orange' if reliability > 0.5 else 'red'\n            st.markdown(f\"- ::{color}[â—]:: [{doc['title']}]({doc['url']}) - {doc.get('authority', 'Unknown Authority')}\")\n            st.caption(f\"Reliability: {reliability:.1%} | Last Updated: {doc.get('last_updated', 'Unknown')}\")\n```\n\n3. Create evidence transparency visualization:\n```python\nimport plotly.graph_objects as go\n\ndef create_evidence_graph(results: Dict[str, Any]):\n    \"\"\"Create a visual graph showing evidence relationships\"\"\"\n    \n    # Extract nodes and edges from GraphRAG results\n    nodes = results.get('graph_nodes', [])\n    edges = results.get('graph_edges', [])\n    \n    # Create network graph using Plotly\n    edge_trace = go.Scatter(\n        x=[], y=[],\n        line=dict(width=0.5, color='#888'),\n        hoverinfo='none',\n        mode='lines')\n    \n    node_trace = go.Scatter(\n        x=[], y=[],\n        mode='markers+text',\n        hoverinfo='text',\n        marker=dict(\n            showscale=True,\n            colorscale='YlGnBu',\n            size=10,\n            colorbar=dict(\n                thickness=15,\n                title='Reliability Score',\n                xanchor='left',\n                titleside='right'\n            )\n        )\n    )\n    \n    # Add graph to Streamlit\n    st.subheader(\"ðŸ”— Evidence Graph\")\n    st.plotly_chart(fig, use_container_width=True)\n```\n\n4. Update main Streamlit app flow:\n```python\n# Main app update\ndef main():\n    st.title(\"GraphRAG-Powered Fact Checking\")\n    st.markdown(\"### Transparent Compliance Verification for Investors\")\n    \n    # Input section\n    claim_text = st.text_area(\"Enter claim to fact-check:\", height=100)\n    \n    # Advanced options\n    with st.expander(\"Advanced Options\"):\n        show_graph = st.checkbox(\"Show Evidence Graph\", value=True)\n        min_reliability = st.slider(\"Minimum Source Reliability\", 0.0, 1.0, 0.5)\n        compliance_frameworks = st.multiselect(\n            \"Compliance Frameworks\",\n            [\"GDPR\", \"CCPA\", \"ISO 27001\", \"SOC 2\", \"HIPAA\"]\n        )\n    \n    if st.button(\"Fact Check with GraphRAG\"):\n        with st.spinner(\"Analyzing with GraphRAG...\"):\n            # Call GraphRAG API\n            response = requests.post(\n                \"http://localhost:8000/fact-check-graphrag\",\n                json={\n                    \"claim\": claim_text,\n                    \"min_reliability\": min_reliability,\n                    \"frameworks\": compliance_frameworks\n                }\n            )\n            \n            if response.status_code == 200:\n                results = response.json()\n                \n                # Display results\n                display_fact_check_results(results)\n                display_source_citations(results)\n                \n                if show_graph:\n                    create_evidence_graph(results)\n                \n                # Export functionality\n                st.download_button(\n                    \"Download Full Report\",\n                    data=json.dumps(results, indent=2),\n                    file_name=\"graphrag_fact_check_report.json\",\n                    mime=\"application/json\"\n                )\n```\n\n5. Add investor-focused metrics dashboard:\n```python\ndef display_investor_metrics(results: Dict[str, Any]):\n    st.subheader(\"ðŸ“Š Investor Confidence Metrics\")\n    \n    col1, col2, col3 = st.columns(3)\n    \n    with col1:\n        compliance_score = results.get('overall_compliance_score', 0)\n        st.metric(\"Compliance Score\", f\"{compliance_score:.1%}\")\n    \n    with col2:\n        evidence_completeness = results.get('evidence_completeness', 0)\n        st.metric(\"Evidence Completeness\", f\"{evidence_completeness:.1%}\")\n    \n    with col3:\n        regulatory_alignment = results.get('regulatory_alignment', 0)\n        st.metric(\"Regulatory Alignment\", f\"{regulatory_alignment:.1%}\")\n```",
        "testStrategy": "1. Test the Streamlit UI with mock GraphRAG API responses containing compliance evidence and citations\n2. Verify all UI components render correctly with sample data including regulatory documents and reliability scores\n3. Test error handling when GraphRAG API is unavailable or returns incomplete data\n4. Validate the evidence graph visualization with various node/edge configurations\n5. Test the export functionality to ensure JSON reports contain all displayed information\n6. Verify responsive design works on different screen sizes for investor presentations\n7. Load test with complex compliance evidence containing multiple frameworks and citations\n8. Test filtering and sorting of citations by reliability score\n9. Validate that all investor metrics calculate correctly from the GraphRAG response data\n10. End-to-end test with actual GraphRAG service to ensure proper integration",
        "status": "pending",
        "dependencies": [
          11
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Create integration testing and deployment pipeline for GraphRAG fact-check system",
        "description": "Establish comprehensive integration testing framework and deployment pipeline for the GraphRAG fact-check system, ensuring end-to-end functionality between factblock-retrieval GraphRAG API and fact-check investor demo with Railway deployment support",
        "details": "1. Create integration test suite for GraphRAG fact-check pipeline:\n```python\n# tests/integration/test_graphrag_factcheck_integration.py\nimport pytest\nimport requests\nimport time\nfrom unittest.mock import patch\nimport docker\nimport os\n\nclass TestGraphRAGFactCheckIntegration:\n    @classmethod\n    def setup_class(cls):\n        # Start required services\n        cls.docker_client = docker.from_env()\n        cls.neo4j_container = cls.docker_client.containers.run(\n            'neo4j:5.14.0',\n            detach=True,\n            ports={'7474/tcp': 7474, '7687/tcp': 7687},\n            environment={'NEO4J_AUTH': 'neo4j/testpassword123'}\n        )\n        time.sleep(10)  # Wait for Neo4j to start\n        \n    def test_end_to_end_fact_check_flow(self):\n        # Test complete flow from fact-check request to GraphRAG retrieval\n        test_claim = 'GDPR requires data deletion within 30 days'\n        \n        # 1. Submit fact-check request to investor demo API\n        response = requests.post(\n            'http://localhost:3000/api/fact-check',\n            json={'claim': test_claim}\n        )\n        assert response.status_code == 200\n        \n        # 2. Verify GraphRAG API was called\n        result = response.json()\n        assert 'verdict' in result\n        assert 'confidence' in result\n        assert 'sources' in result\n```\n\n2. Create Railway deployment configuration:\n```yaml\n# railway.toml\n[build]\nbuildCommand = \"pip install -r requirements.txt && python -m pytest tests/\"\nstartCommand = \"gunicorn -w 4 -b 0.0.0.0:$PORT app:app\"\n\n[deploy]\nhealthcheckPath = \"/health\"\nhealthcheckTimeout = 300\nrestartPolicyType = \"on-failure\"\nrestartPolicyMaxRetries = 3\n\n[env]\nPYTHON_VERSION = \"3.11\"\nNEO4J_URI = \"bolt://neo4j-service.railway.internal:7687\"\n```\n\n3. Implement CI/CD pipeline with GitHub Actions:\n```yaml\n# .github/workflows/deploy-graphrag.yml\nname: Deploy GraphRAG Fact-Check System\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    services:\n      neo4j:\n        image: neo4j:5.14.0\n        env:\n          NEO4J_AUTH: neo4j/testpassword123\n        ports:\n          - 7474:7474\n          - 7687:7687\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: '3.11'\n    \n    - name: Install dependencies\n      run: |\n        pip install -r requirements.txt\n        pip install pytest-cov\n    \n    - name: Run integration tests\n      env:\n        NEO4J_URI: bolt://localhost:7687\n        NEO4J_USER: neo4j\n        NEO4J_PASSWORD: testpassword123\n      run: |\n        pytest tests/integration/ -v --cov=src\n    \n  deploy:\n    needs: test\n    runs-on: ubuntu-latest\n    if: github.ref == 'refs/heads/main'\n    \n    steps:\n    - uses: actions/checkout@v3\n    \n    - name: Install Railway CLI\n      run: npm install -g @railway/cli\n    \n    - name: Deploy to Railway\n      env:\n        RAILWAY_TOKEN: ${{ secrets.RAILWAY_TOKEN }}\n      run: |\n        railway up --service graphrag-api\n        railway up --service fact-check-demo\n```\n\n4. Create deployment validation script:\n```python\n# scripts/validate_deployment.py\nimport requests\nimport time\nimport sys\n\ndef validate_deployment(base_url: str, max_retries: int = 5):\n    endpoints = [\n        f'{base_url}/health',\n        f'{base_url}/fact-check-graphrag',\n        f'{base_url}/api/fact-check'\n    ]\n    \n    for endpoint in endpoints:\n        for attempt in range(max_retries):\n            try:\n                response = requests.get(endpoint, timeout=10)\n                if response.status_code == 200:\n                    print(f'âœ“ {endpoint} is healthy')\n                    break\n            except:\n                if attempt == max_retries - 1:\n                    print(f'âœ— {endpoint} failed after {max_retries} attempts')\n                    sys.exit(1)\n                time.sleep(5)\n    \n    # Test end-to-end flow\n    test_response = requests.post(\n        f'{base_url}/api/fact-check',\n        json={'claim': 'Test claim for deployment validation'}\n    )\n    \n    if test_response.status_code == 200:\n        print('âœ“ End-to-end fact-check flow working')\n    else:\n        print('âœ— End-to-end test failed')\n        sys.exit(1)\n\nif __name__ == '__main__':\n    validate_deployment(sys.argv[1] if len(sys.argv) > 1 else 'https://factcheck.railway.app')\n```\n\n5. Configure environment-specific settings:\n```python\n# src/config/deployment.py\nimport os\nfrom enum import Enum\n\nclass DeploymentEnv(Enum):\n    LOCAL = 'local'\n    STAGING = 'staging'\n    PRODUCTION = 'production'\n\nclass DeploymentConfig:\n    def __init__(self):\n        self.env = DeploymentEnv(os.getenv('DEPLOYMENT_ENV', 'local'))\n        \n        self.config_map = {\n            DeploymentEnv.LOCAL: {\n                'neo4j_uri': 'bolt://localhost:7687',\n                'graphrag_api_url': 'http://localhost:8000',\n                'fact_check_api_url': 'http://localhost:3000'\n            },\n            DeploymentEnv.STAGING: {\n                'neo4j_uri': os.getenv('STAGING_NEO4J_URI'),\n                'graphrag_api_url': os.getenv('STAGING_GRAPHRAG_URL'),\n                'fact_check_api_url': os.getenv('STAGING_FACTCHECK_URL')\n            },\n            DeploymentEnv.PRODUCTION: {\n                'neo4j_uri': os.getenv('PROD_NEO4J_URI'),\n                'graphrag_api_url': os.getenv('PROD_GRAPHRAG_URL'),\n                'fact_check_api_url': os.getenv('PROD_FACTCHECK_URL')\n            }\n        }\n    \n    def get_config(self):\n        return self.config_map[self.env]\n```",
        "testStrategy": "1. Local integration testing:\n   - Start all services locally using docker-compose\n   - Run full integration test suite verifying data flow from fact-check API through GraphRAG to Neo4j\n   - Test error scenarios (Neo4j down, GraphRAG timeout, invalid claims)\n   - Verify response consistency between old OpenAI and new GraphRAG implementations\n\n2. Deployment pipeline testing:\n   - Test GitHub Actions workflow on feature branch\n   - Verify Railway deployment configuration with test project\n   - Validate health checks respond correctly\n   - Test rollback procedures\n\n3. Performance testing:\n   - Load test with 100 concurrent fact-check requests\n   - Monitor GraphRAG API response times\n   - Verify Neo4j connection pooling under load\n   - Test Railway auto-scaling triggers\n\n4. End-to-end validation:\n   - Deploy to staging environment\n   - Run validation script against staging URLs\n   - Test with production-like data volume\n   - Verify monitoring and logging integration\n\n5. Security testing:\n   - Validate API authentication between services\n   - Test Neo4j credentials are properly secured\n   - Verify no sensitive data in logs\n   - Test Railway environment variable encryption",
        "status": "pending",
        "dependencies": [
          11,
          7,
          10
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-07-05T03:20:16.399Z",
      "updated": "2025-07-05T20:03:44.253Z",
      "description": "Tasks for master context"
    }
  }
}